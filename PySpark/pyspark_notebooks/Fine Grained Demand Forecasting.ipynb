{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Demand Forecasting--\n",
    "\n",
    "The objective of this notebook is to illustrate how we might generate a large number of fine-grained forecasts at the store-item level in an efficient manner leveraging the distributed computational power of Databricks.  For this exercise, we will make use of an increasingly popular library for demand forecasting, [FBProphet](https://facebook.github.io/prophet/), which we will load into the notebook session associated with a cluster running Databricks 6.0 or higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fbprophet library\n",
    "#dbutils.library.installPyPI('FBProphet', version='0.6') # find latest version of fbprophet here: https://pypi.org/project/fbprophet/\n",
    "#dbutils.library.installPyPI('holidays','0.9.12') # this line is in response to this issue with fbprophet 0.5: https://github.com/facebook/prophet/issues/1293\n",
    "\n",
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Data\n",
    "\n",
    "For our training dataset, we will make use of 5-years of store-item unit sales data for 50 items across 10 different stores.  This data set is publicly available as part of a past Kaggle competition and can be downloaded [here](https://www.kaggle.com/c/demand-forecasting-kernels-only/data). \n",
    "\n",
    "Once downloaded, we can uzip the *train.csv.zip* file and upload the decompressed CSV to */FileStore/tables/demand_forecast/train/* using the file import steps documented [here](https://docs.databricks.com/data/tables.html#create-table-ui). Please note when performing the file import, you don't need to select the *Create Table with UI* or the *Create Table in Notebook* options to complete the import process.\n",
    "\n",
    "With the dataset accessible within Databricks, we can now explore it in preparation for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow\n",
    "pyarrow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fbprophet\n",
      "  Using cached fbprophet-0.6.tar.gz (54 kB)\n",
      "Requirement already satisfied: Cython>=0.22 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (0.29.15)\n",
      "Requirement already satisfied: cmdstanpy==0.4 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (0.4.0)\n",
      "Requirement already satisfied: pystan>=2.14 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (2.19.1.1)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (1.18.1)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (1.0.1)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (3.1.3)\n",
      "Requirement already satisfied: LunarCalendar>=0.0.9 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (0.0.9)\n",
      "Requirement already satisfied: convertdate>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (2.2.0)\n",
      "Requirement already satisfied: holidays>=0.9.5 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (0.10.1)\n",
      "Requirement already satisfied: setuptools-git>=1.2 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from fbprophet) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.4->fbprophet) (2019.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n",
      "Requirement already satisfied: ephem>=3.7.5.3 in /opt/conda/lib/python3.7/site-packages (from LunarCalendar>=0.0.9->fbprophet) (3.7.7.1)\n",
      "Requirement already satisfied: pymeeus<=1,>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from convertdate>=2.1.2->fbprophet) (0.3.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from holidays>=0.9.5->fbprophet) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->fbprophet) (45.2.0.post20200209)\n",
      "Building wheels for collected packages: fbprophet\n",
      "  Building wheel for fbprophet (setup.py) ... \u001b[?25l/^C\n"
     ]
    }
   ],
   "source": [
    "!pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath(\"../pyspark_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create sparksession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pysparko\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.driver.maxResultSize\", \"0\")\n",
    "spark.conf.set(\"spark.rpc.message.maxSize\",\"2000\")\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# structure of the training data set\n",
    "train_schema = StructType([\n",
    "  StructField('date', DateType()),\n",
    "  StructField('store', IntegerType()),\n",
    "  StructField('item', IntegerType()),\n",
    "  StructField('sales', IntegerType())\n",
    "  ])\n",
    "\n",
    "# read the training file into a dataframe\n",
    "train = spark.read.csv(\n",
    "  os.path.join(data_path,'train.csv'), \n",
    "  header=True, \n",
    "  schema=train_schema\n",
    "  )\n",
    "\n",
    "# make the dataframe queriable as a temporary view\n",
    "train.createOrReplaceTempView('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable magic command 'sql'\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: user@system'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql presto://user@localhost:8080/system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing demand forecasting, we are often interested in general trends and seasonality.  Let's start our exploration by examing the annual trend in unit sales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">year</td><td style=\"font-weight: bold\">sales</td></tr><tr><td>2013</td><td>7941243</td></tr><tr><td>2014</td><td>9135482</td></tr><tr><td>2015</td><td>9536887</td></tr><tr><td>2016</td><td>10357160</td></tr><tr><td>2017</td><td>10733740</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "\n",
    "SELECT\n",
    "  year(date) as year, \n",
    "  sum(sales) as sales\n",
    "FROM train\n",
    "GROUP BY year(date)\n",
    "ORDER BY year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very clear from the data that there is a generally upward trend in total unit sales across the stores. If we had better knowledge of the markets served by these stores, we might wish to identify whether there is a maximum growth capacity we'd expect to approach over the life of our forecast.  But without that knowledge and by just quickly eyeballing this dataset, it feels safe to assume that if our goal is to make a forecast a few days, months or even a year out, we might expect continued linear growth over that time span.\n",
    "\n",
    "Now let's examine seasonality.  If we aggregate the data around the individual months in each year, a distinct yearly seasonal pattern is observed which seems to grow in scale with overall growth in sales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">month</td><td style=\"font-weight: bold\">sales</td></tr><tr><td>2013-01-01</td><td>454904</td></tr><tr><td>2013-02-01</td><td>459417</td></tr><tr><td>2013-03-01</td><td>617382</td></tr><tr><td>2013-04-01</td><td>682274</td></tr><tr><td>2013-05-01</td><td>763242</td></tr><tr><td>2013-06-01</td><td>795597</td></tr><tr><td>2013-07-01</td><td>855922</td></tr><tr><td>2013-08-01</td><td>766761</td></tr><tr><td>2013-09-01</td><td>689907</td></tr><tr><td>2013-10-01</td><td>656587</td></tr><tr><td>2013-11-01</td><td>692643</td></tr><tr><td>2013-12-01</td><td>506607</td></tr><tr><td>2014-01-01</td><td>525987</td></tr><tr><td>2014-02-01</td><td>529117</td></tr><tr><td>2014-03-01</td><td>704301</td></tr><tr><td>2014-04-01</td><td>788914</td></tr><tr><td>2014-05-01</td><td>882877</td></tr><tr><td>2014-06-01</td><td>906842</td></tr><tr><td>2014-07-01</td><td>989010</td></tr><tr><td>2014-08-01</td><td>885596</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "\n",
    "SELECT \n",
    "  TRUNC(date, 'MM') as month,\n",
    "  SUM(sales) as sales\n",
    "FROM train\n",
    "GROUP BY TRUNC(date, 'MM')\n",
    "ORDER BY month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating the data at a weekday level, a pronounced weekly seasonal pattern is observed with a peak on Sunday (weekday 0), a hard drop on Monday (weekday 1) and then a steady pickup over the week heading back to the Sunday high.  This pattern seems to be pretty stable across the five years of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">year</td><td style=\"font-weight: bold\">weekday</td><td style=\"font-weight: bold\">sales</td></tr><tr><td>2013</td><td>0</td><td>25788.44230769231</td></tr><tr><td>2013</td><td>1</td><td>17269.69230769231</td></tr><tr><td>2013</td><td>2</td><td>20015.811320754718</td></tr><tr><td>2013</td><td>3</td><td>20150.153846153848</td></tr><tr><td>2013</td><td>4</td><td>21503.19230769231</td></tr><tr><td>2013</td><td>5</td><td>23071.096153846152</td></tr><tr><td>2013</td><td>6</td><td>24532.903846153848</td></tr><tr><td>2014</td><td>0</td><td>29901.0</td></tr><tr><td>2014</td><td>1</td><td>19791.98076923077</td></tr><tr><td>2014</td><td>2</td><td>23179.346153846152</td></tr><tr><td>2014</td><td>3</td><td>23009.471698113208</td></tr><tr><td>2014</td><td>4</td><td>24805.884615384617</td></tr><tr><td>2014</td><td>5</td><td>26344.75</td></tr><tr><td>2014</td><td>6</td><td>28207.423076923078</td></tr><tr><td>2015</td><td>0</td><td>31213.903846153848</td></tr><tr><td>2015</td><td>1</td><td>20787.903846153848</td></tr><tr><td>2015</td><td>2</td><td>24089.96153846154</td></tr><tr><td>2015</td><td>3</td><td>24174.346153846152</td></tr><tr><td>2015</td><td>4</td><td>25763.67924528302</td></tr><tr><td>2015</td><td>5</td><td>27640.826923076922</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "\n",
    "SELECT\n",
    "  YEAR(date) as year,\n",
    "  CAST(DATE_FORMAT(date, 'u') as Integer) % 7 as weekday,\n",
    "  --CONCAT(DATE_FORMAT(date, 'u'), '-', DATE_FORMAT(date, 'EEEE')) as weekday,\n",
    "  AVG(sales) as sales\n",
    "FROM (\n",
    "  SELECT \n",
    "    date,\n",
    "    SUM(sales) as sales\n",
    "  FROM train\n",
    "  GROUP BY date\n",
    " ) x\n",
    "GROUP BY year, CAST(DATE_FORMAT(date, 'u') as Integer) --, CONCAT(DATE_FORMAT(date, 'u'), '-', DATE_FORMAT(date, 'EEEE'))\n",
    "ORDER BY year, weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are oriented to the basic patterns within our data, let's explore how we might build a forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Build a Forecast\n",
    "\n",
    "Before attempting to generate forecasts for individual combinations of stores and items, it might be helpful to build a single forecast for no other reason than to orient ourselves to the use of FBProphet.\n",
    "\n",
    "Our first step is to assemble the historical dataset on which we will train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">ds</td><td style=\"font-weight: bold\">y</td></tr><tr><td>2013-01-01 00:00:00</td><td>13</td></tr><tr><td>2013-01-02 00:00:00</td><td>11</td></tr><tr><td>2013-01-03 00:00:00</td><td>14</td></tr><tr><td>2013-01-04 00:00:00</td><td>13</td></tr><tr><td>2013-01-05 00:00:00</td><td>10</td></tr><tr><td>2013-01-06 00:00:00</td><td>12</td></tr><tr><td>2013-01-07 00:00:00</td><td>10</td></tr><tr><td>2013-01-08 00:00:00</td><td>9</td></tr><tr><td>2013-01-09 00:00:00</td><td>12</td></tr><tr><td>2013-01-10 00:00:00</td><td>9</td></tr><tr><td>2013-01-11 00:00:00</td><td>9</td></tr><tr><td>2013-01-12 00:00:00</td><td>7</td></tr><tr><td>2013-01-13 00:00:00</td><td>10</td></tr><tr><td>2013-01-14 00:00:00</td><td>12</td></tr><tr><td>2013-01-15 00:00:00</td><td>5</td></tr><tr><td>2013-01-16 00:00:00</td><td>7</td></tr><tr><td>2013-01-17 00:00:00</td><td>16</td></tr><tr><td>2013-01-18 00:00:00</td><td>7</td></tr><tr><td>2013-01-19 00:00:00</td><td>18</td></tr><tr><td>2013-01-20 00:00:00</td><td>15</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql  \n",
    "SELECT\n",
    "    CAST(date as TIMESTAMP) as ds,\n",
    "    sales as y\n",
    "FROM train\n",
    "WHERE store=1 AND item=1\n",
    "ORDER BY ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to aggregate data to date (ds) level\n",
    "sql_statement = '''\n",
    "  SELECT\n",
    "    CAST(date as TIMESTAMP) as ds,\n",
    "    sales as y\n",
    "  FROM train\n",
    "  WHERE store=1 AND item=1\n",
    "  ORDER BY ds\n",
    "  '''\n",
    "\n",
    "# assemble dataset in Pandas dataframe\n",
    "history_pd = spark.sql(sql_statement).toPandas()\n",
    "\n",
    "# drop any missing records\n",
    "history_pd = history_pd.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import the fbprophet library, but because it can be a bit verbose when in use, we will need to fine-tune the logging settings in our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-73ea423f9c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# disable informational messages from fbprophet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'py4j'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "from fbprophet import Prophet\n",
    "import logging\n",
    "\n",
    "# disable informational messages from fbprophet\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our review of the data, it looks like we should set our overall growth pattern to linear and enable the evaluation of weekly and yearly seasonal patterns. We might also wish to set our seasonality mode to multiplicative as the seasonal pattern seems to grow with overall growth in sales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "model = Prophet(\n",
    "  interval_width=0.95,\n",
    "  growth='linear',\n",
    "  daily_seasonality=False,\n",
    "  weekly_seasonality=True,\n",
    "  yearly_seasonality=True,\n",
    "  seasonality_mode='multiplicative'\n",
    "  )\n",
    "\n",
    "# fit the model to historical data\n",
    "model.fit(history_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, let's use it to build a 90-day forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataset including both historical dates & 90-days beyond the last available date\n",
    "future_pd = model.make_future_dataframe(\n",
    "  periods=90, \n",
    "  freq='d', \n",
    "  include_history=True\n",
    "  )\n",
    "\n",
    "# predict over the dataset\n",
    "forecast_pd = model.predict(future_pd)\n",
    "\n",
    "display(forecast_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did our model perform? Here we can see the general and seasonal trends in our model presented as graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_fig = model.plot_components(forecast_pd)\n",
    "display(trends_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, we can see how our actual and predicted data line up as well as a forecast for the future, though we will limit our graph to the last year of historical data just to keep it readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fig = model.plot( forecast_pd, xlabel='date', ylabel='sales')\n",
    "\n",
    "# adjust figure to display dates from last year + the 90 day forecast\n",
    "xlim = predict_fig.axes[0].get_xlim()\n",
    "new_xlim = ( xlim[1]-(180.0+365.0), xlim[1]-90.0)\n",
    "predict_fig.axes[0].set_xlim(new_xlim)\n",
    "\n",
    "display(predict_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** This visualization is a bit busy. Bartosz Mikulski provides [an excellent breakdown](https://www.mikulskibartosz.name/prophet-plot-explained/) of it that is well worth checking out.  In a nutshell, the black dots represent our actuals with the darker blue line representing our predictions and the lighter blue band representing our (95%) uncertainty interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection is useful, but a better way to evaulate the forecast is to calculate Mean Absolute Error, Mean Squared Error and Root Mean Squared Error values for the predicted relative to the actual values in our set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from datetime import date\n",
    "\n",
    "# get historical actuals & predictions for comparison\n",
    "actuals_pd = history_pd[ history_pd['ds'] < date(2018, 1, 1) ]['y']\n",
    "predicted_pd = forecast_pd[ forecast_pd['ds'] < date(2018, 1, 1) ]['yhat']\n",
    "\n",
    "# calculate evaluation metrics\n",
    "mae = mean_absolute_error(actuals_pd, predicted_pd)\n",
    "mse = mean_squared_error(actuals_pd, predicted_pd)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# print metrics to the screen\n",
    "print( '\\n'.join(['MAE: {0}', 'MSE: {1}', 'RMSE: {2}']).format(mae, mse, rmse) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FBProphet provides [additional means](https://facebook.github.io/prophet/docs/diagnostics.html) for evaluating how your forecasts hold up over time. You're strongly encouraged to consider using these and those additional techniques when building your forecast models but we'll skip this here to focus on the scaling challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Scaling Model Training & Forecasting\n",
    "\n",
    "With the mechanics under our belt, let's now tackle our original goal of building numerous, fine-grain models & forecasts for individual store and item combinations.  We will start by assembling sales data at the store-item-date level of granularity:\n",
    "\n",
    "**NOTE**: The data in this data set should already be aggregated at this level of granularity but we are explicitly aggregating to ensure we have the expected data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_statement = '''\n",
    "  SELECT\n",
    "    store,\n",
    "    item,\n",
    "    CAST(date as date) as ds,\n",
    "    SUM(sales) as y\n",
    "  FROM train\n",
    "  GROUP BY store, item, ds\n",
    "  ORDER BY store, item, ds\n",
    "  '''\n",
    "\n",
    "store_item_history = (\n",
    "  spark\n",
    "    .sql( sql_statement )\n",
    "    .repartition(sc.defaultParallelism, ['store', 'item'])\n",
    "  ).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data aggregated at the store-item-date level, we need to consider how we will pass our data to FBProphet. If our goal is to build a model for each store and item combination, we will need to pass in a store-item subset from the dataset we just assembled, train a model on that subset, and receive a store-item forecast back. We'd expect that forecast to be returned as a dataset with a structure like this where we retain the store and item identifiers for which the forecast was assembled and we limit the output to just the relevant subset of fields generated by the Prophet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "result_schema =StructType([\n",
    "  StructField('ds',DateType()),\n",
    "  StructField('store',IntegerType()),\n",
    "  StructField('item',IntegerType()),\n",
    "  StructField('y',FloatType()),\n",
    "  StructField('yhat',FloatType()),\n",
    "  StructField('yhat_upper',FloatType()),\n",
    "  StructField('yhat_lower',FloatType())\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model and generate a forecast we will leverage a Pandas user-defined function (UDF).  We will define this function to receive a subset of data organized around a store and item combination.  It will return a forecast in the format identified in the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )\n",
    "def forecast_store_item( history_pd ):\n",
    "  \n",
    "  # TRAIN MODEL AS BEFORE\n",
    "  # --------------------------------------\n",
    "  # remove missing values (more likely at day-store-item level)\n",
    "  history_pd = history_pd.dropna()\n",
    "  \n",
    "  # configure the model\n",
    "  model = Prophet(\n",
    "    interval_width=0.95,\n",
    "    growth='linear',\n",
    "    daily_seasonality=False,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    seasonality_mode='multiplicative'\n",
    "    )\n",
    "  \n",
    "  # train the model\n",
    "  model.fit( history_pd )\n",
    "  # --------------------------------------\n",
    "  \n",
    "  # BUILD FORECAST AS BEFORE\n",
    "  # --------------------------------------\n",
    "  # make predictions\n",
    "  future_pd = model.make_future_dataframe(\n",
    "    periods=90, \n",
    "    freq='d', \n",
    "    include_history=True\n",
    "    )\n",
    "  forecast_pd = model.predict( future_pd )  \n",
    "  # --------------------------------------\n",
    "  \n",
    "  # ASSEMBLE EXPECTED RESULT SET\n",
    "  # --------------------------------------\n",
    "  # get relevant fields from forecast\n",
    "  f_pd = forecast_pd[ ['ds','yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\n",
    "  \n",
    "  # get relevant fields from history\n",
    "  h_pd = history_pd[['ds','store','item','y']].set_index('ds')\n",
    "  \n",
    "  # join history and forecast\n",
    "  results_pd = f_pd.join( h_pd, how='left' )\n",
    "  results_pd.reset_index(level=0, inplace=True)\n",
    "  \n",
    "  # get store & item from incoming data set\n",
    "  results_pd['store'] = history_pd['store'].iloc[0]\n",
    "  results_pd['item'] = history_pd['item'].iloc[0]\n",
    "  # --------------------------------------\n",
    "  \n",
    "  # return expected dataset\n",
    "  return results_pd[ ['ds', 'store', 'item', 'y', 'yhat', 'yhat_upper', 'yhat_lower'] ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot taking place within our UDF, but if you compare the first two blocks of code within which the model is being trained and a forecast is being built to the cells in the previous portion of this notebook, you'll see the code is pretty much the same as before. It's only in the assembly of the required result set that truly new code is being introduced and it consists of fairly standard Pandas dataframe manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call our UDF to build our forecasts.  We do this by grouping our historical dataset around store and item.  We then apply our UDF to each group and tack on today's date as our *training_date* for data management purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "results = (\n",
    "  store_item_history\n",
    "    .groupBy('store', 'item')\n",
    "    .apply(forecast_store_item)\n",
    "    .withColumn('training_date', current_date() )\n",
    "    )\n",
    "\n",
    "results.createOrReplaceTempView('new_forecasts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We we are likely wanting to report on our forecasts, so let's save them to a queriable table structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- create forecast table\n",
    "create table if not exists forecasts (\n",
    "  date date,\n",
    "  store integer,\n",
    "  item integer,\n",
    "  sales float,\n",
    "  sales_predicted float,\n",
    "  sales_predicted_upper float,\n",
    "  sales_predicted_lower float,\n",
    "  training_date date\n",
    "  )\n",
    "using delta\n",
    "partitioned by (training_date);\n",
    "\n",
    "-- load data to it\n",
    "insert into forecasts\n",
    "select \n",
    "  ds as date,\n",
    "  store,\n",
    "  item,\n",
    "  y as sales,\n",
    "  yhat as sales_predicted,\n",
    "  yhat_upper as sales_predicted_upper,\n",
    "  yhat_lower as sales_predicted_lower,\n",
    "  training_date\n",
    "from new_forecasts;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how good (or bad) is each forecast?  Using the UDF technique, we can generate evaluation metrics for each store-item forecast as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# schema of expected result set\n",
    "eval_schema =StructType([\n",
    "  StructField('training_date', DateType()),\n",
    "  StructField('store', IntegerType()),\n",
    "  StructField('item', IntegerType()),\n",
    "  StructField('mae', FloatType()),\n",
    "  StructField('mse', FloatType()),\n",
    "  StructField('rmse', FloatType())\n",
    "  ])\n",
    "\n",
    "# define udf to calculate metrics\n",
    "@pandas_udf( eval_schema, PandasUDFType.GROUPED_MAP )\n",
    "def evaluate_forecast( evaluation_pd ):\n",
    "  \n",
    "  # get store & item in incoming data set\n",
    "  training_date = evaluation_pd['training_date'].iloc[0]\n",
    "  store = evaluation_pd['store'].iloc[0]\n",
    "  item = evaluation_pd['item'].iloc[0]\n",
    "  \n",
    "  # calulate evaluation metrics\n",
    "  mae = mean_absolute_error( evaluation_pd['y'], evaluation_pd['yhat'] )\n",
    "  mse = mean_squared_error( evaluation_pd['y'], evaluation_pd['yhat'] )\n",
    "  rmse = sqrt( mse )\n",
    "  \n",
    "  # assemble result set\n",
    "  results = {'training_date':[training_date], 'store':[store], 'item':[item], 'mae':[mae], 'mse':[mse], 'rmse':[rmse]}\n",
    "  return pd.DataFrame.from_dict( results )\n",
    "\n",
    "# calculate metrics\n",
    "results = (\n",
    "  spark\n",
    "    .table('new_forecasts')\n",
    "    .filter('ds < \\'2018-01-01\\'') # limit evaluation to periods where we have historical data\n",
    "    .select('training_date', 'store', 'item', 'y', 'yhat')\n",
    "    .groupBy('training_date', 'store', 'item')\n",
    "    .apply(evaluate_forecast)\n",
    "    )\n",
    "results.createOrReplaceTempView('new_forecast_evals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will likely want to report the metrics for each forecast, so we persist these to a queriable table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "create table if not exists forecast_evals (\n",
    "  store integer,\n",
    "  item integer,\n",
    "  mae float,\n",
    "  mse float,\n",
    "  rmse float,\n",
    "  training_date date\n",
    "  )\n",
    "using delta\n",
    "partitioned by (training_date);\n",
    "\n",
    "insert into forecast_evals\n",
    "select\n",
    "  store,\n",
    "  item,\n",
    "  mae,\n",
    "  mse,\n",
    "  rmse,\n",
    "  training_date\n",
    "from new_forecast_evals;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have constructed a forecast for each store-item combination and generated basic evaluation metrics for each.  To see this forecast data, we can issue a simple query (limited here to product 1 across stores 1 through 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT\n",
    "  store,\n",
    "  date,\n",
    "  sales_predicted,\n",
    "  sales_predicted_upper,\n",
    "  sales_predicted_lower\n",
    "FROM forecasts a\n",
    "WHERE item = 1 AND\n",
    "      --store IN (1, 2, 3, 4, 5) AND\n",
    "      date >= '2018-01-01' AND\n",
    "      training_date=current_date()\n",
    "ORDER BY store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for each of these, we can retrieve a measure of help us assess the reliability of each forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT\n",
    "  store,\n",
    "  mae,\n",
    "  mse,\n",
    "  rmse\n",
    "FROM forecast_evals a\n",
    "WHERE item = 1 AND\n",
    "      training_date=current_date()\n",
    "ORDER BY store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Fine Grained Demand Forecasting",
  "notebookId": 2692780145731095,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
